

# PARAKEET Discussion


#### **Wednesday, May 5th at 5 pm CEST / 11 am EDT / 8 am PDT.**

Scribe: Erik Anderson

Open discussion to review technical details and open[ GitHub issues](https://github.com/WICG/privacy-preserving-ads/issues) for the PARAKEET proposal.

If you want to participate, please make sure you join the WICG: [https://www.w3.org/community/wicg/](https://www.w3.org/community/wicg/)

The setup will be a Microsoft Teams meeting: [Click here to join the meeting](https://teams.microsoft.com/l/meetup-join/19%3ameeting_OTk5MzYwZmUtZjNmMi00MTRjLWFjOTItYzdkM2Q3NmIyYTZl%40thread.v2/0?context=%7b%22Tid%22%3a%2272f988bf-86f1-41af-91ab-2d7cd011db47%22%2c%22Oid%22%3a%2279442ff5-1ecc-4579-99d9-39d3ded3c43d%22%7d)

Or call in (audio only): [Find a local number](https://dialin.teams.microsoft.com/8551f4c1-bea3-441a-8738-69aa517a91c5?id=400194663) Phone Conference ID: 655 825 985# 


## Agenda



1. **Allow malicious ads prevention scripts to scan Parakeet ads for malicious activities- [#17](https://github.com/WICG/privacy-preserving-ads/issues/17)**
2. **Request to outline the total expected latency before an ad is rendered via Parakeet- [#18](https://github.com/WICG/privacy-preserving-ads/issues/18)**
3. **How do we expect Programmatic Guaranteed and other Private auctions to work through Parakeet -[#19](https://github.com/WICG/privacy-preserving-ads/issues/19)**


## Queue - raise your hand in Teams


## Attendees (sign yourself in):



1. Erik Anderson (Microsoft)
2. Kelda Andeson
3. Brendan Riordan-Butterworth (IAB Tech Lab / eyeo GmbH)
4. Russell Stringham (Adobe)
5. Brian May (dstillery)
6. Saurabh Shah (Walmart)
7. Paul Selden (OpenX)
8. Nishanth Chandran (Microsoft)
9. Chris Starr (Hearst Autos)
10. Michael Kleber (Chrome)
11. Ryan Arnold (P&G)
12. Phil Eligio (EPC)
13. Andrew Pascoe (NextRoll)
14. Wendell Baker (Verizon Media)
15. Kiran Gopinath (BanterX)
16. Przemyslaw Iwanczak (RTB House) 
17. Grzegorz Lyczba 
18. Anthony Molinaro
19. Larry Price 
20. Duncan
21. Aleksei Gorbushin
22. Sasha
23. Gang Wang
24. Matt Wilson
25. Mike Pisula 
26. Raj 
27. Hooman 
28.  Pranay 
29. Aleksei Gorbushin
30. Matt Wilson

Notes:

Kelda: &lt;intro>

Allow malicious ads prevention scripts to scan Parakeet ads for malicious activities- [#17](https://github.com/WICG/privacy-preserving-ads/issues/17)



*   Mehul: right now there are third-party scripts that run to check for malicious ads, e.g. malware installs, page takeover, popup. The question is what is the plan for this? \
 \
One of the answers we had is that the ads go through the SSP and they can check the ad bundles. But one of the concerns is the maliciousness shows up only when run on a client, e.g. based on IP detection.
*   Erik: &lt;explains his comment at [https://github.com/WICG/privacy-preserving-ads/issues/17#issuecomment-831601315](https://github.com/WICG/privacy-preserving-ads/issues/17#issuecomment-831601315)>
*   It is more questions than answers, that would be beneficial to understand 
*   What do the scripts feel like they need to look at today?
*   Is it the DOM content? What are their outputs?
*    One Of the concerns w/ outputting a signal  @ run time is k-anonymity & other privacy checks
*   If we have signals coming out of the fenced frame it makes it challenging to confidently assert that there isn’t some join happening there.
*   What are the capabilities that these checks need to have? Can we make them more robust?
*   Maybe we could have trusted piece of script?
*   Initial thinking for reporting we would do a reporting API - we figure out what signal is needed.
*   Aggregation service - look at latency concerns to identifying and pulling of ad
*   Many options but we need to think what Should be done.
*   Mehul: maybe fenced frame eliminates some current risks? E.g. can limit on CPU/memory/etc. There are some different companies here as well.
*   Kleber: we’ve been thinking about this a bit differently and would like Robin to weigh in. With bundles being provided here, the typical cloaking risk that Robin mentions here where ads behave one way when they think they’re being scanned but then behave differently when in a rendered environment. That normally happens via different resources being served. That’s normally been server-side logic to decide what to serve to the browser in the “maliciously exploit the situation.” That particular threat is really mitigated by a fenced frame rendering in which there is some limited ability and you need to know what all of the resources are that will be loaded in advance. \
 \
The way we thought of this on the FLEDGE side is ads need to declare the rendering URL, e.g. an ad bundle that needs to be downloaded, or something downloaded in advance and with a fingerprint put on it so that the resources needs to be the same at rendering time as they were at scan time. A variety of ways that could be ensured. Fenced frame makes it much more feasible to make sure things can’t change.
*   Mehul: that is fairly aligned with what Erik and I put in the issue. Could also look at resource usage. Hearing back from Pranay and Robin as well as some existing entities that run this sort of system would be valuable.
*   Mehul: with Pranay here, to summarize: the fenced frame and ad prefetch mechanisms limits what the ad can do. The other capability if not downloaded as a web bundle that was mentioned is that there could be a fingerprint put on the ad in advance to confirm the ad hasn’t changed. What other capabilities are needed?
*   Pranay: the thing is that we are not security specialists to highlight the scenarios that are possible. When evaluating various solutions that are out there, a lot of malicious ad behavior is done when the ad makes a network call to load additional assets which are hard to predict beforehand. Since the ad needs to be downloaded as a bundle and can’t make additional network requests, the risk is mitigated. Not being a specialist in this area, we do want one or more of these specialist companies to evaluate the risks.
*   Mehul: with more clarity, we can evaluate new capabilities.
*   Pranay: we can get back to you after talking to the contacts at some of these companies. One possibility is, the way safe frame API works is certain page contexts that certain providers need, they can access the info via a safe frame API. But, yeah, more to come as we discuss with these companies.

How do we expect Programmatic Guaranteed and other Private auctions to work through Parakeet -[#19](https://github.com/WICG/privacy-preserving-ads/issues/19)



*   Mehul: this is about whether the deal ID is available as part of the PARAKEET request. I was hoping to get additional clarity. There are private marketplace deals. Basically, DSP buys at a premium rate for a publisher and how they know it is the deal ID. If the deal ID goes through to the DSP it can handle it. \
 \
How specific is the deal ID? Would have an interaction with k-anonymity check.
*   Pranay: I reached out to some of my business counterparts. The way I understand these work, a publisher and advertiser discuss a deal beforehand. “These are some special audiences we have on our side, and when the user matches that audience we’ll put in the deal ID and you’ll know it’s the one we were talking about.” The deal might be done such that it might be only 1000 users in a deal; no mechanism today to control that. We can make an assumption that the deal ID may be granular and specific to a small number of users; just saying there is nothing on it today.
*   Mehul: does deal ID work together with interest signals? Or would deal ID always win? If not correlated, the direct sold flow could be done. If both are needed, that gets very interesting. This is similar to when the Google SSP team talked about the key-value pair.
*   Pranay: Google system does a KVP and it maps to a deal ID and that gets passed along which is less sensitive.
*   Sasha: a buyer can layer on a 3rd-party audience and buy the intersection, but I’m not sure that’s a use case we necessarily need to account for. It’s not desirable from my point of view since it does allow for identification in a way and it limits a small ? already. “Audience added from one side only” is probably fine.
*   Mehul: a few things-- if it passes k-anon threshold for domain+this. It would be good to get a little more in terms of the granularity of things. We did put a user signal from the publisher with the same privacy checks to make sure it’s not identifiable. We have said if nothing passes the k-anon threshold, you can specify what’s a priority and we can drop all of the 3p context. E.g. during NFL time, the premium associated might be higher than anything else. The idea was that you provide the priority and if we see k-anon bars aren’t passing, the publisher can tell us to keep the contextual info as-is.
*   Kleber: From the Chrome proposal side in FLEDGE, the answer to this would be that the publisher 1p audiences and similar deals would be part of the contextual request that doesn’t get any of the 3p signals and would not be part of the on-device audience and would instead be part of the standard server RTB flow without user interest groups at all. In the previous PARAKEET meeting, we talked about there being a purely contextual ad request even before the PARAKEET flow happens, so there’s a possibility of having two requests-- the first contextual with all of the thing you’re talking about and then after that a second request that does go through the PARAKEET server that has both the coarsened contextual information and the appropriately anonymized user-specific interest group information. So, the possibility of there being priorities assigned by the publisher so one request can play both of those roles is interesting and I haven’t thought about it. Playing potential buyers against each other where they are interested in different signals is very complex; would like to hear from publishers about what they’d need and a lot of potential complexity to think about.
*   Mehul: if a priority signal is given, then the publisher knows what’s the most valuable already. The reason we built the primitive was if the publisher builds something based on verticals, e.g. car section wants to make/model/year to get a bigger premium from those brands, so they don’t want a retargeted ad to dilute the value or relevance of the slot. If some of these things are a little more crisp, there’s a place to make it available. If both signals are used together, though, it needs to pass a k-anon threshold.
*   Pranay: 3p signal along with deal ID is not something we are aware of, but probably the buy side is going to be better able to answer the question.
*   Mehul: okay, we’ll keep it open and consider reaching out to some DSPs.

Request to outline the total expected latency before an ad is rendered via Parakeet- [#18](https://github.com/WICG/privacy-preserving-ads/issues/18)



*   Mehul: the question is what’s the expected latency before the ad renders via PARAKEET.
*   Pranay: I’ve been trying to break down the sequential steps that happen before the ad comes back and renders in the browser. Because of anonymity we need, PARAKEET acts as a proxy which is one more layer on top of what we have today. And then MPC computations are heavy, I understand on the server, but still heavy. Say top ad positions on a publisher page-- if you don’t show an ad quickly, the user will scroll down and it will hurt viewability and other aspects. I’ve listed at very broad strokes the number of steps I think will happen before an ad comes back and I think it feels very heavy.
*   Mehul: I want to separate out step 3, secure compute with MaCAW. With PARAKEET flow (1) and (2) are happening. We will put a tight latency budget on things, perhaps 20ms or so for the compute itself, not necessarily the network. It should be fairly quick and then the flow is similar with the SSP and multiple DSPs happening on the server side. A single request is going out and a lot of DSPs can be connected to each other to pick the winner ad. The anonymity process in between should be limited; I don’t have a concrete number yet but we will. \
 \
For secure compute, we put the latency of the calculation in the explainer. It itself is quite expensive. With a scoped set of features it can be done in 200ms. If the number of features needed to compute the outcome, the complexity quickly goes up much higher-- in the explainer the table shows it taking multiple seconds. We have a couple of modes-- don’t expose all of the signals encrypted while making sure they aren’t privacy-revealing still. And then we talked about possibly having a helper that can reduce the cost of the flow. We also added that to the table, but it adds an additional step. To be upfront, the step 3 for the latency budget is not completely defined yet; we’d like to try it out and we’re trying to ad it to our own ad serving flows with a reasonably large-scale model. Encourage other SSPs and DSPs to try it with their models using the info in the explainer to evaluate how expensive it is for their models to run. If you can share bottlenecks with us, we can evaluate how to optimize it.

	&lt;shows MaCAW document; we’re talking about steps 4 and 5 in the diagram>



*   Pranay: obviously you understand the concern here and we’ll look for more info and updates. There is also issue 4 or 5 where I made a comment about how can we parallelize network calls on the page? Since there will be latency concerns… the way the flow has been described is that we wait for the direct ad to complete and then we compete it with the interest group ads coming through FLEDGE and the final auction happens on the browser. If these solutions have latency concern, we shouldn’t do this serially but should instead be as parallel as possible. If the publisher is happy with the direct ad back from the ad server, we could tell Parakeet to cancel the request because we’re already happy.
*   Mehul: that’s fair, we need to make sure the capability is the publisher’s choice. If the pre-sold threshold/floor dependency is known upfront or it can be issued without a floor then it can totally be done in parallel. We trigger it when the publisher calls the API. If you have all of those signals as soon as you have the page response coming back, you can kick it off right on page load.
*   Brian May: with respect to the timing issue, giving publishers a way to tell the browser what their tolerance is for response time may be useful here. Beyond that, giving publishers a way to signal the browser about what they want to happen could be useful. The publisher might say “I only want a contextual flow here, don’t do anything on the FLEDGE/behavioral side here.”
*   Mehul: the latency restrictions are fair that there should be some timeouts, either collapsing the ad or rendering a direct sold should be doable. Re: publisher preference, we called out to let the publisher tell us the priority, but what other details you have in mind would help.
*   Brian: the one on the top of my head is the “let me do contextual and don’t do behavioral” for a publisher that would have a more constrained set of things to concern themselves with. In the contextual signal, there are publisher-only things that don’t communicate anything about the user which could be provided by the publisher to have a privacy-safe ad experience. Exploring what level of signal can be provided by the publisher without concerns for privacy violations would be interesting; the type of signals that can be provided without worrying about what can be attached to the user.
*   Mehul: if no 3p signals being passed, there are no restrictions in terms of what can be sent.
*   Erik: I think the idea is that the publisher might want to leverage PARAKEET to tell the user that they serve more private ads because PARAKEET is providing privacy guarantees. Correct, Brian?
*   Brian: Yes, essentially. One more thing about that-- if we come up with a set of signals that don’t have any chance of being joined to the user that would be helpful so they can understand what the working set is.
*   Aswath: we talked about IAB taxonomy at some point which is a similar idea.
*   Kleber: want to point out that the discussion on TURTLEDOVE included a bunch of talk that publishers often wish to control latency budgets not on a one-size-fits-all basis where the PARAKEET server picks a cutoff or even publisher-by-publisher, but instead they want buyers to have different latency budgets. I’m not in a position to speak about why, but that’s what we heard. This is tending in the direction of some configuration payload where the publisher might want to have a lot of flexibility in telling PARAKEET how to handle things, which would include time budgets. A fairly rich control channel between publisher and PARAKEET would be valuable.
*   Mehul: in PARAKEET, if they don’t want to configure it via PARAKEET, it still goes to the SSP and they could set up the logic on the SSP side ahead of time which then goes out to other SSPs or DSPs. They could keep that logic working as-is. But yes, there is some need for a configuration specification for how to control timeouts, priorities of signals, etc. Thanks for the link to the TURTLEDOVE repo issue.
*   Pranay: I do want to break this into three areas: (1) there is an ad server which a publisher works with to get the direct ads, no programmatic, (2) call going to PARAKEET where we are talking about a configuration object from the publisher where it can say “I don’t want interest group or 3p data to be used, only contextual info such as deal ID.” I’m not aware of a situation where we get a sponsorship ad where we committed to showing only one ad on the home page for 24 hours; we don’t need any automated buying to happen-- we’re clear on what to show. Or the advertiser is good enough we never get a better ad. (3) We want to see what the PARAKEET side is doing. I don’t think there’s any situation outside of deal ID where we wouldn’t want the interest groups too. For the latency/timeouts, we have that for all sorts of transactions we do such as if SSP doesn’t come back with a bid in time; that flexibility is expected in future solutions. The crux of this particular issue is not the timeout or flexibility, but what the actual latency end-to-end will be and if it would meet the thresholds for timeouts that publishers expect. \
 \
I proposed a comment in issue #4 which we can discuss in the next call is if publishers can have flexibility to parallelize the calls and then have the final decision happen in the browser itself. We’d be cutting down on the latency by having things happen in parallel.
*   Mehul: in parallel is a good idea as long as you can provide the right configuration. To directly answer the question re: latency, the secure compute will add latency, but PARAKEET itself shouldn’t add a lot of latency. The proxy being reasonably distributed shouldn’t add extra latency and the communication to the SSPs/DSPs should be “cloud to cloud” with low latency. We also have described a fallback bid mechanism. Ad slot shouldn’t go blank.
*   Phil Eligio: I don’t know how to resolve this overall in terms of managing the latency. Parallel case is interesting. From a business perspective, latency is extremely commercially sensitive. In terms of adding a 100ms here or there can affect things quite a bit. That may put some limit on what can be done via MPC. We can test how many features, how it’s handled, but if we end up above a certain number, it is quite commercially sensitive. To add on to the question or point of managing latency by different dimensions, I can add weight to that being important. For instance, different positions on pages and deals negotiated with different buyers with expectations of delivery; publishers need to manage for those types of scenarios. High level, but wanted to make sure it continues to be considered.
*   Brian: a couple of things-- first, adding on to what Michael Kleber said, publisher configuration based on my 1p data, I may want specific tolerances for that browser vs. a different. Longer latency for browsers I’m familiar with that will stick on the page for a while and a lower latency for other browsers I don’t know anything about. Second, with browsers getting involved in ad delivery process, want to think about how publishers and advertisers get info about ad delivery at a more macro level. If a publisher gets a lot of timeouts due to partners they’re working with or other things that go into ad serving, I don’t know what feedback mechanism they get to figure out that an ad times out consistently on a channel.
*   Mehul: I want to further point out that in PARAKEET flow, SSP is collecting bids and can instrument which buyers are timing out. But we’ll still look into other meaningful thresholds and controls, what happens during a violation, and how to report back to the right party that something is timing out.

Mehul: want to discuss issue #15 quickly. Gave some feedback here, anything else you’re looking for Michael?

Michael: it’s clear now, you might want to consider updating the main text. Especially with the different priorities for different signals thing we talked about today.

Mehul: sure, we will look into adding info into the explainer for clarity. For everyone else, the issue is about if the publisher has context on the user, does it go in _c_ or _s_? In last call, I said it could go through _s_. We later internally debated and it’s possible to pass the 1p signal in a contextual flow or add an ad interest for the user like advertisers do. Both go through anonymization, so both are possible and choice will vary based on how the publisher thinks about their signal, protecting it, and how they want to use it. Everyone: feel free to go through it and add comments.

**Two requests for everyone**-- we published a polyfill library that allows ad-side developers to try it out. Add ad interests, see if the flows would work. Try it out in your developer settings, not production yet. Review the APIs and try to set it up. The second request is the secure compute flows-- we put out a compiler and a reference in the explainer. You can try it out in your developer environment with your bidder model or auction logic and try to measure the latency and what challenges you may face. Those two would be very helpful.
