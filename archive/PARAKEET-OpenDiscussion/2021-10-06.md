


# PARAKEET Discussion


####Wednesday, Oct 6th at 5 pm CEST / 11 am EDT / 8 am PDT.

Scribe: Kelda Anderson

Open discussion to review technical details and open[ GitHub issues](https://github.com/WICG/privacy-preserving-ads/issues) for the PARAKEET proposal.

If you want to participate, please make sure you join the WICG: [https://www.w3.org/community/wicg/](https://www.w3.org/community/wicg/)

The setup will be a Microsoft Teams meeting: [Click here to join the meeting](https://teams.microsoft.com/l/meetup-join/19%3ameeting_OTk5MzYwZmUtZjNmMi00MTRjLWFjOTItYzdkM2Q3NmIyYTZl%40thread.v2/0?context=%7b%22Tid%22%3a%2272f988bf-86f1-41af-91ab-2d7cd011db47%22%2c%22Oid%22%3a%2279442ff5-1ecc-4579-99d9-39d3ded3c43d%22%7d)

Or call in (audio only): [Find a local number](https://dialin.teams.microsoft.com/8551f4c1-bea3-441a-8738-69aa517a91c5?id=400194663) Phone Conference ID: 655 825 985# 


## Agenda


#### 
    Open Q&A and [Github Issues](https://github.com/WICG/privacy-preserving-ads/issues)



* Retargeting clarification - request participation for Oct 6th
    * PARAKEET & Retargeting · Issue [#34](https://github.com/WICG/privacy-preserving-ads/issues/34)
        * [#38](https://github.com/WICG/privacy-preserving-ads/issues/38)
* Detecting Invalid Traffic with Anonymization and Aggregated Reporting · Issue [#35](https://github.com/WICG/privacy-preserving-ads/issues/35) - request participation for Oct 6th
* PARAKEET: Clarify HTTPS requirements · Issue [#36](https://github.com/WICG/privacy-preserving-ads/issues/36) - request participation for Oct 6th
1. 


## Queue - raise your hand in Teams


## Attendees (sign yourself in):



1. Kelda Anderson (Microsoft)
2. Mehul Parsana (Microsoft)
3. Erik Anderson (Microsoft)
4. Sergey Tumakha (Microsoft)
5. David Turner (Google Ad Traffic Quality)
6. Brian May (dstillery)
7. Fred Bastello (Index Exchange)
8. Angelina Eng (IAB & IAB Tech Lab)
9. Andrew Pascoe (NextRoll)
10. Valentino Volonghi (NextRoll)
11. Brendan Riordan-Butterworth (eyeo GmbH)
12. Wendell Baker (Yahoo)
13. Joel Meyer (OpenX)
14. Joel Pfeiffer (Microsoft)
15.  Jonasz Pamuła (RTB House)
16. Phil Eligio (EPC)
17. Per Bjorke (Google Ad Traffic Quality)
18. Aditya Desai (Google)
19. Bill Landers (Xandr)
20. Brad Rodriguez (Magnite)
21. Jeff Wieland (Magnite)


### Notes:

Start with Retargeting clarification - request participation for Oct 6th

-PARAKEET & Retargeting · Issue [#3](https://github.com/WICG/privacy-preserving-ads/issues/34)4

**Mehul -**
-Jonasz I see your question, can we start by clarifying that

**Jonasz -**

So let's start with 34. Thank you for your answers to give everyone the context. If not everybody is familiar with the issue. 

The issue is about how much of the re-targeting signal that was initially added to the browser to the interest groups will be available in the final request.

When writing this issue I was thinking about PARAKEET ,but judging from your answer, it seems like you are the target solution that you have in mind is something that involves parakeet MaCAW and perhaps a bit more.

**Mehul-**

 Fair enough, I think the client side in answer. I did use MPC, but let's go through, step by step. I think let's I think you clarified the question. 

How much of the retargeting but the first I try to clarify some of the assumption which might not be super clear about where we are trying to achieve anonymization at award granularity.

The First thing is, yes, it is true. It is a tradeoff between how specific the request could be and how much power at runtime you have to decide the bid and auction.

Fundamental, I think that's the key. If you think key difference between the FLEDGE. And that's why there are two alternatives, right? If you look at the FLEDGE versus the PARAKEET one you can push as fine grained as possible with the individual signal when you try to aggregate them then you need this bid server and plus.

**Jonasz -**

Yes

**Mehul-**

Auction construct, for the PARAKEET - what if we do a privacy tradeoff up front in the request? Then you can do a lot of things on a server side in terms of computing a  bid auction & retrieval everything. Now that's a tradeoff. 

A couple of clarification, the one thing is very important to think about is the targeting signal, which survives the PARAKEET request. So, you could include extra ads in the response based on the hint you get. And since the DSP develops a targeting signal, they also understand the bit of a correlation between targeting signal.

 If you want to think like people who are interested in laptop likely be interested in specific Lenovo model or specific Dell … , even though you don't have retargeting signal, you could include those ads and then there's a client side workflow or MPC. Though MPC is a little bit out in the future.

The client-side work looking invalidate those ads which has a strict restriction of remarketing segment which are two specific and work with the hints. 

We were trying to not complex the bid and auction flow and see if we can make it work on the server side so that you can evaluate probably very large number of ads on a server rather than having a client-side computation.

**Jonasz -**

 I see, so finalize ads is mentioned somewhere in the PARAKEET explainer? I was searching for it but I couldn't find it.

 
**Mehul-**

 yes it is in the polyfill implementation GH info, I’ll add a pointer in the notes: \


**Jonasz-**

Makes sense, thanks for your answer in the issue and for clarifications. I think when it comes to the hints that you mentioned.  

The biggest difficulty would be if the Popularity thresholds are applied to all interests at the same time.

  

If they are applied to interests coming from a single interest group that signal will be much more finely grained if they are applied to all the interests from all the advertisers, I think this signal might very quickly disappear, basically in the final request.

 

**Mehul-**

It seems like we are applying a popularity threshold, but it's not directly a popularity threshold on an advertiser targeting signal… We're sort of doing this.

A clustering style proposal in a current explainer, which will not necessary enforce the popularity if you think because it automatically tries to leverage the correlation between those interest group. If there are very random projection of the interest group, you are right. It basically will disappear because every user you are trying to identify why random bits. So two way to think about this is if the targeting signals are based on the user true user interest.

It is correlated based on how user is looking what user is looking for then and if there are sufficient user then lot of common targeting signal will survive.

If there is a hash of this, some user ID or something which are sort of random high entropy bits, all of them will disappear. And that's by construct.

This is one of the thing in a clustering-  we aim to do, it is sort of open source that part of the differential privacy tradeoff thing. People can try a lot of different algorithms overtime and achieve the tradeoff between what is getting lost, how to incorporate the value of that.

It's not that we are making the one proposal there, but to acknowledge, yes, that will be a challenge. If they are extremely diverse, set of interest.

No correlation between them likely that the orthogonal kind of interest group will disappear. The mainstream interests of the user will survive that that process.

**Jonasz -**

I see, the way I was thinking about it, you mentioned that the advertiser could add interests at different levels of granularity. The highest level of granularity .. to my understanding, is that advertisers name.

So, if each advertiser only stores its name as the interest, and let's say we live in a world with 10,000 advertisers. Each user has three advertisers. Then we already have 10,000 choose three, which is a lot of unique vectors.

And so that was my main worry that this signal might disappear entirely after this clustering phase.

If the clustering or the popularity threshold is applied per advertiser, not for all interests, then this worry of mine I think goes away.

**Mehul -**

There are two parts to this

The first is if you assume all interest group to be completely random. You are totally right. So current targeting signal you have. You should try to cluster them and see how correlated they are we are seeing or some good sign on that front when, but that's targeting data we have which is not same as what you might be thinking right?

The second thing is highest granularities signal to think about is the category sort of interest of the user, not the domain name he visited like travel domain like destination and exit period is a finest granularity. But if you go higher level than user want to travel or vacation, holiday vacation is the highest granularity. Not visiting a booking.com is the highest  And of course, the value is lost in between.

When this goes higher up, but that's just one of the idea, but the best way to do it is sort of how you analyze your targeting data at a user level, which is across advertiser cluster them and see how correlated they are, what information is getting lost if you take a centroid of the cluster and how valuable information is getting lost and what does it mean to you, and that will give us good data to work on to kind of figure out what is the better way to think. Now the second point.

To answer about per advertiser signal we are currently calling a DSP, if you look at the code flow there is a one request going to ad server with all the targeting signal, they have access to.

If you fragment that in advertiser call, I think it's a scalability challenge.

If the DSP is working for potentially 1000 advertisers and if the receiver thousand different call, it has a two challenge. One is those many ad requests need to be solved at a runtime to participate in auction.

And second, more importantly, is that you're calling the same time. So in from a privacy point of view, the ads server still got a thousand requests.

You know in a very short amount of time, so you still know all the information together, it neither survives the scalability test nor privacy test. So that's why we had to kind of think them together, but there could be a novel solution there. What you're trying to say is if you fragment the request by advertiser, would it be better?

If there is a way to do that, We are happy to accommodate and understand that.

**Jonasz -**

One quick comment to your previous point. If we define interests as let's say a category of products, laptops or headphones that would have a certain side effect that is not very fortunate for us.

When we serve ads, we try not to use or we do not use the data coming from a certain advertiser to serve ads from the other advertisers.

So it seems like the advertisers name has to be part of the signal that we sent out for at least the ad selection.

**Mehul -**

That is fair, So I think that is a similar restriction that most of their network would have, so that's where I it will be great to run the clustering on your own if you try to group the users. When you look at all advertiser signals.

And then see what kind of cluster it results into your end. I can put out a document what kind of clustering we are thinking of, but it is not only proposal. Again, the whole objective was to open up this kind of tradeoff framework. People can plug in a different algorithm there, see the trade off, and it's acceptable by user. Acceptable by monetization impact and then it becomes a scalable infrastructure from that point.

**Jonasz-**

If there is any document that we can look and comment on, then I'll be happy to do so.

We can also moving on toIssue 38,

When you think about the retargeting signal, it is really being used for two things.

1st.- The bid value computation. - it seems like with the idea of a finalized AD or with the general idea that was presented in MaCAW, we will be able to do it very well with PARAKEET or MaCAW by computing the bid value on the precise signals device side or using MPC.

2nd - The second use case for the retargeting signals is ad selection.

The retargeting use case could happen at the time of join, Add Interest Group and that will basically remove the problem of having to make the anonymized retargeting signals granular enough. Maybe we can actually store the other would like to display in the joint Ad Interest Group and that would solve the  selection process &then compute the precise bid value using MaCAW and that seems  it could be a viable solution for retargeting.

**Mehul-**

So that is a very interesting idea we have two parts which I would like to get more thoughts if we can actually kind of further extrapolate that the one part is a skill like we the MaCAW, even though it seems promising at a current compute cost, it is going to be expensive. So, we think that scoring for fine grained bid up to few 10s of ads to little bit on the higher side can be done. But if you're trying to evaluate a few hundred ads for every placement, that seems quite an expensive step.

**Jonasz –**

I see yes, so it seems like we have a couple of open issues.

·        One is calculating precise bid value.

·        One is picking the right ad.

·        One accounting and one is model training.

Our approach so far was to focus on each of these problems separately. So assuming we can deliver an optimal model, how would we like to use it for computation model training?

**Mehul-**

But they are all connected. They are all connected issues which point to the privacy issue. Future in a future right? You've solved the serving problem correctly. Then you punt it down to reporting or building. If you put it down you solve bidding in a closed box. Then you punt it down to reporting if you solve reporting with some noise then you put it down to training. The four problem are super connected.

**Jonasz-**

They are, yes they are.

So I see assuming that all the other problems are solved and we only focus on the problem of ad selection. Would you say it is a good idea to put the selected ad in the interest group? Or would you have some worries about that approach?

**Mehul -**

If other problems are solved with a reasonable privacy constraint, I don't see any direct issue with that as long as this ad is serving to at least reasonable number of users, we will still think that this interest is available to reasonable number of users and the user has way to opt out and disable the ad if they like to.

If these two are satisfied and there is no in future when we are considering this ad, we are not exposing a very fine-grained signal which can make it privacy challenge, it seems a possible potential and that could sit in between the FLEDGE and PARAKEET world.

It is an interesting construct, but I would like you to extrapolate a bit further - like how would it work, end to end.

We will be happy to discuss in a way if there is a way to figure that out.

**Jonasz-**

Sure, I think that that answer answers my issue completely. Thank you.

**Mehul-**

Michael has hand raised Michael, do you want to go?

**Michael-**

Yeah, just since you mentioned the FLEDGE version, I'll just say what our view is on exactly the question that Jonasz is talking about.

From the fledged point of view, I think that, exactly. The idea of the ad being built into the interest group or being a something known in advance to the interest group is definitely an option that we have liked from the beginning and  exactly as you said, having a key anonymity sort of constraint that makes sure that this same ad is being shown to a large enough number of people is the way we can have the ad carried by the browser and tie to the interest group without it immediately giving away the users cross site identity as soon as you do reporting on what ad showed.

So the kind of variant - the hard question is just how you calculate the bid and eligibility questions, and that's where FLEDGE proposes on device computation and parakeet proposes the kind of proxied noised signals to the server. Having that just affect the choice of which interest groups are bidding and how much they're bidding but leaving the question of which ad is going to display as something that comes from -Browser thing seems like a more appealing way to split the job up. I think I agree with Jonasz on these issues. 

**Mehul -**

OK, great, I think we have some more detail thought to be followed up on.

 

Additional kind of flow- How would the big computation follow? How would it be model training might evolve this thought further and support both. Potentially where somebody would like to keep that on the browser side? That's fine. Somebody wants to include that via that proxy. Anonymized requests at runtime it because we got clearly seeing the tradeoff between the privacy, staleness, the targeting and it might work along together.

But I think Andrew has his hand raised.

**Andrew Pascoe -**

 Yeah, I've had my hand raised for a little while.

I kind of like what Michael just said. Like I, I want to support their take on this.

From our perspective- a purely selfish DSP perspective, we generally view the benefits of PARAKEET as opposed to fledge as being that we don't have to change as much about our RTB building infrastructure because the privacy is taken care of for us and we can just respond to requests in the way that we normally would. We just don't get as much detail or it's with noise added. But the targeting use case -  a lot of advertisers create campaigns that are, abandoned cart campaigns, which are naturally small, you know you’ve seen ads. They know you’ve added this thing to your cart  & come back to buy it or whatever.

You know the reporting use case and things like that. I think that at least at next role we've just kind of come to terms with the fact that if an ad is only shown to three people, that that might not show up in reporting, but also, it's going to be fractions of a fraction of a penny & isn't that big of a deal.

And at least through some of the some of the other proposals through the aggregate reporting API where there's this kind of idea of like prefixed trees to kind of get more summarized results.

We don't think that billing is going to be a major concern for this type of stuff. Obviously hitting some type of K anonymity so that when the browser calls out to render, the ad needs to be supported.

But I think that yeah, for us we would  be very concerned if we were not able to support  targeting case for specific audiences that have taken very specific actions.

**Mehul -**

Thank you Andrew. I think we should evaluate both proposals in terms of what targeting it can support. That's why I explained that as more like a hint of like you can say website visitor and you can pass in.

Adding PARAKEET past few 10s of Ads and then invalidate that with the two targeting signal not on the client side.

I think we hear you. I mean, this is the kind of a debate we would like to have in kind of understanding what all can be done if we retrieved the early contribute computation can still work via privacy Anonymized request. What signal it needed.

 There are some more thoughts to be followed up …Can the bid also be default?

We need to kind of clear the thing too. If we are serving so few 10s of user, whether the billing would happen really uh to a fraction if the K Anonymized mid threshold is 100 and then if there is a 10% noise added or 5% noise to be added.

What it means from a charging perspective, but that's one part of it. But the bit model training another aspect where type to it. So that's why I kind of include it. It's just that we are delaying this together signal of publisher and advertiser which are required end to end serving. You're just pushing it down the down the funnel where it surface is.

 

**Andrew –**

Yep.

Per Bjorke - 

Wanted to comment that when it comes to billing/accounting it is a non trivial issue. For example if you have an error in your system and then after the fact you got to go and correct some of it, it may be a certain segment of traffic. You need to have data to support a correction. If you are auditing. If you're challenged, if you're sued, whatever it may be, you may need to go into the details and document it, justify it so that needs to be thought through. But that's for another day.

**Mehul-**

I agree yes, and that's why I was saying that they're pushing the issue down. Yes, I agree. I mean, that requires a reasonable level of signal to make that happen.

But if up front you where serving in a privacy protective way, then the signal you log on a server side is reasonably sufficient to so. Again, I think we can take an action item Extrapolate thought further because it's in the beginning from there and we can improve on it.

David, you had something to add. And Angelina, I think I see two hands. 

**Angelina Eng-**

I think the the latency issue is definitely going to be a concern across the board  

Most advertisers close their books or because we have to, they go through intermediaries like agencies. Publishers are already complaining that they don't get paid on time already, and so any delayed reporting as well as any discrepancies and reconfiguring of the stats will cause quite a big havoc.

So I just want to reiterate that, thank you.

**Mehul -**

Yeah, thank you.

Thanks for discussing that. We can move on to a second issue 35 which is about detecting invalid traffic with anonymization and aggregated reporting.

I don't think we have all the answers figured out. 

**David Turner -**

Yeah, I'm here and we agreed that we and lots of folks that we've engaged with also don’t have all the answers here, but we wanted to use this time to at least discuss basic requirements related to invalid traffic just to make sure we're all in agreement on that. So I'm joined with Per, we're both from the Google traffic quality team.

We've been working in this space for quite a long time.

But I guess before we kind of talk about related requirements when it comes to any of these privacy preserving APIs that does anonymization or aggregation, I do think there was also a comment last time which we weren't here related to Trust tokens, and we certainly think it can help. It won't cover all of the requirements that we need here and there. I think you have some additional thoughts on this.

**Per Bjorke (Google) –**

 

Yes, so in general invalid traffic is an adverse detecting. Invalid traffic is an adversary relationship. You are fighting bad guys for the most part, not always. Some of it isn't but  it's an arms race. We make advances. They make advanced as the bypasses and so the game goes.

And an important thing to think about here is that in order to keep up that cycle we rely on.

 1st initial time sequence & the ability to do manual analysis.

We doing manual analysis to understand, investigate, analyze to figure out what's going on. You may find some type of an attack pattern. You may find some actors involved in the attack & then you can train and develop models, filters, whatever you want to call it. Whatever you use to automatically detect that and deploy it.

So just to facilitate this, you need to have the ability to both do the manual analysis on an unpredictable set of attributes and signals, and you need ability to deploy your run failure against machine learning and less sophisticated denialists in production system to block the traffic that is bad & the key for this is often an signals value.

High entropy signals in many cases because you need to segment the bad from the good traffic. So for example, if you're sitting on a desktop computer now, or a laptop computer or not.

You may be doing legitimate valid activity in your browser. Visiting New York Times, for example. So at the same time, you could have an infection on a computer that is tapping into your browser, driving a browser, and also doing invalid traffic. Visiting newer types at the same time, and you want to separate it out.

Likewise, if you're traffic coming from a data center, some of that is perfectly valid.

Bank of America employees sitting at the desk in the office if they if they got it back in the office will typically go through a VPN service and many of us to do the same thing. While you have a lot of emulators sitting in data centers, so there's finding invalid traffic is all about  slicing and dicing and segmented traffic down to fine level granularity.

 

For that purpose, it runs typically counter or opposite to what the privacy efforts often are the same signals that we are trying to protect.

Those  for protecting privacy are the ones that are often very informative for invalid traffic detection.

There is room for a lot of advancements like trust token can be and in the banishment in this space. But it's important to realize there's no silver bullet in space. Trust token is and very fine piece of technology to transfer some information from 1 entity to another entity in a privacy respecting way.

But it doesn't detect invalid traffic by itself, there’s nothing inherent in trust token that magically detects invalid traffic. If for example Google or Microsoft knows that some traffic is valid will reduce are an activity coming from a browser is valid?

 

That's the thing that's important to recognize in this space. There is no silver bullet. We can find lots of tools to do that, but fundamentally, if we don't address invalid traffic detection in a systematic way to enable that, not one single API is going to be able to solve it. So, we have to basically take this head on and say, how are we going to facilitate invalid traffic detection?

 

And I think that's the proposal that David posted about. That is one thought about how this can be done, but we do want to make sure that people understand that there is no one single API that we solve this because the bad guys are going to figure that out then.

David, you want to talk a little bit about the proposal. This structure.

**David –**

Most of our on our invalid traffic detection purely on server side, event level logs and of course anything that's going to do anonymization or aggregation will eliminate many of those high entropy signals and make it challenging to do to detect traffic. So, if we can't really do much server side, it seems like our only options could be maybe do something fully in the browser, but that seems to also be a huge risk.

And very challenging as well as reverse engineering risk of deploying models in the browser. So maybe a nice compromise is can we somehow do something more within trusted servers?

One pitch here is to facilitate the allowed human analysis that's needed for finding new attacks, being able to respond to advertiser escalations.

Could we fully maybe encrypt all these events for the purposes of invalid traffic? Maybe there could be some auditing aspect as well to ensure that only members of invalid traffic team contain issue queries against some aggregation service on top of those encrypt data events, but I think that's kind of the the main aspect is, could there be some kind of allowed access in a trusted server environment and then maybe also add noise on top of the aggregated results, otherwise double?

This has its own issues as well. To really ensure that only about numbers of invalid traffic team have access to that. So, the main page again is maybe fully encrypt all that. Let it run in trust the server that humans issued or queries there & then you could also deploy or train your models.

You still need to have those human analysts still evaluate that traffic and then also deploy. The model is also trusted server. So maybe like within the PARAKEET anonymization server all that could happen there that it could encrypt these events and then also run these models there. So that's at least what the idea is, but more important again as what pair said is basically but basically these requirements to ensure that invalid traffic is part of these privacy preserving APIs.

**Per -**

And just sit tight back to this brief topping we had about accounting one of the elements of comprehensive detection, invalid traffic detection, health fraud detection, as after the fact collection, because no matter how good you are, you know not going to catch everything in real time, so you have to be able to catch things after the fact and go back and correct it. So, at that point you also need to be able to do fairly detailed level building corrections, and that ties into what we brought up briefly earlier about.

Imagine you have any system bug. You also need to be able to correct some set of your billing.

There is another one that is tricky to do without the event level data. Not impossible, but tricky to do without it.

**Mehul -**

Fair enough, I think this is this is a valid concern and like we been actively thinking about what are the different ways that fraud detection will be impacted. And I also agree that trust token is not answer to everything. That aspect and specifically even the transferring a trust where trust token has other challenges like tokens are issued and then device start doing a fraud then those tokens are still valid and still working. So,  there are there are potential gap there as well.

-I think we should have a more detailed deep dive discussion on that. I can have a fraud team involved and then we can kind of workout the way I was looking at a proposal is there. Is this a yellow box of EVT protection provider are sort of super user, right? They have to read even though they are working with this helper servers, sort of their getting as granular data as they want, right? That's how I was interpreting that proposal.

The other thing to what it means and terms of and also would every ad server will have their own EVT production provider, or are these considered as sort of third party or sort of external entities? What it means? What kind of output they will produce?

 So we don't need to go back to buyer and seller, right? It's just segregate correction goes back or some more proof need to go back so there are there are like lot of things to be followed up after that, but thank you for sharing  

** Per –**

We should have a more in depth conversation. Everyone may not be interested in that, but it may be good to have a group type driver focus on that a little bit more and then come back to the product group and report back on.

We are very happy to sit down with anyone who's interested in brainstorming and discussing.

**Brian May-**

How do we go about raising our hands or or indicating interest?



* 

People interested in the invalid traffic/ad fraud discussion:



* Andrew Pascoe (NextRoll)
* Valentino Volonghi (NextRoll)
* Per Bjorke and David Turner (Google)
* Angelina Eng (IAB & IAB Tech Lab)
* Brian May (dstillery)
* Jonasz Pamuła (RTB House)

**Mehul-** 

We also had a third issue we did touch last time Erik did provide HTTPS requirement detail answer Eric, do you have anything to add there?

**Erik Anderson-**

No, I think it was just clarification. Like most powerful platform APIs, everything is going to need a secure context including this.

 

**Brad Rodriguez -**

-We have filed an issue. Just looking for some clarity on where and open RTB.

On the exact parameter, but it's equivalent to the site ID or the ad placement ID that's in the proprietary to Parakeet?

There's probably a lot of places it could go, and I'm not sure that you would have the answer right now, but, just wanted to raise that as an open question. That's sort of blocking us from trying to move forward with some testing.

-

**Mehul -**

Uh, yeah, so let me get back to you on that Brad. I don't have an answer right away for you- that's issue #37,

Yeah, I will answer on the on the GitHub and we will finalize the field if you do. If you do have a preference, then do let us know. But otherwise from our perspective Edge is going to give a signal to any of the valid.

**Brad -**

OK, great yeah, yes, it's 37 time confirmed.

Brad Rodriguez (Magnite) (Guest)

But I'll check with the team and see if there's a preference, so we'll add it to the ticket.

**Mehul -**

Sounds good, thank you.

**Brad -**

Maybe still time for coffee.

**Mehul -**

Thank you everyone.
