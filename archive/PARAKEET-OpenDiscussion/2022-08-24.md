<!-- Output copied to clipboard! -->

<!-----

Yay, no errors, warnings, or alerts!

Conversion time: 0.916 seconds.


Using this Markdown file:

1. Paste this output into your source file.
2. See the notes and action items below regarding this conversion run.
3. Check the rendered output (headings, lists, code blocks, tables) for proper
   formatting and use a linkchecker before you publish this page.

Conversion notes:

* Docs to Markdown version 1.0β33
* Wed Nov 23 2022 12:13:54 GMT-0800 (PST)
* Source doc: Copy of PARAKEET - August 24, 9:10 AM
----->



# PARAKEET Discussion


#### Wednesday, August 24th at 5 pm CET / 11 am EDT / 8 am PDT

Scribe: Erik

Open discussion to review technical details and open[ GitHub issues](https://github.com/WICG/privacy-preserving-ads/issues) for the PARAKEET proposal.

If you want to participate, please make sure you join the WICG: [https://www.w3.org/community/wicg/](https://www.w3.org/community/wicg/)

The setup will be a Microsoft Teams meeting: [Click here to join the meeting](https://teams.microsoft.com/l/meetup-join/19%3ameeting_OTk5MzYwZmUtZjNmMi00MTRjLWFjOTItYzdkM2Q3NmIyYTZl%40thread.v2/0?context=%7b%22Tid%22%3a%2272f988bf-86f1-41af-91ab-2d7cd011db47%22%2c%22Oid%22%3a%2279442ff5-1ecc-4579-99d9-39d3ded3c43d%22%7d)

Or call in (audio only): [Find a local number](https://dialin.teams.microsoft.com/8551f4c1-bea3-441a-8738-69aa517a91c5?id=400194663). Phone Conference ID: 655 825 985# 


## Agenda



* [PARAKEET with Noisy Ranking update](https://github.com/WICG/privacy-preserving-ads/blob/main/NoisyRanking.md)
    * We are in the process of measuring & evaluating results from noisy ranking outlined in the Parakeet proposal. \
We will share the results with the community as soon as they are available.
* [Native ads update](https://github.com/WICG/privacy-preserving-ads/blob/35da18bbda75708780109a522778829355debf94/FencedNativeRendering.md)
* Open Q&A and [Github Issues](https://github.com/WICG/privacy-preserving-ads/issues)
    * [ PARAKEET with Noisy Ranking: Advertiser isolation #47 ](https://github.com/WICG/privacy-preserving-ads/issues/47)<span style="text-decoration:underline;"> </span>


## Queue - raise your hand in Teams.


## Attendees (sign yourself in):



1. Erik Anderson (Microsoft Edge)
2. Sergey Tumakha (Microsoft Ads)
3. Brian May (dstillery)
4. Joel Pfeiffer (MSFT)
5. Andrew Pascoe (NextRoll)
6. Bartosz Marcinkowski (RTB House)
7. Davis Gilton (Microsoft)
8. Zhisong Wang (MSFT)
9. Jonasz Pamuła (RTB House)
10. (Walmart)
11. Wendell Baker (Yahoo)
12. Alex Cone (IAB Tech Lab)
13. Philip Baumann (Microsoft)
14. Eubert Go (Microsoft)
15. Sivakanth Gopi (Microsoft Research)
16. 
17. 
18.  


### Notes:


### [PARAKEET with Noisy Ranking update](https://github.com/WICG/privacy-preserving-ads/blob/main/NoisyRanking.md)

Joel Pfeiffer: Gopi (and Sergey Yekhanin) from Microsoft Research is here. MSR gave us “magic numbers” to help inform us about why things are private enough.

Sivakanth Gopi: Hi– sharing my screen–

&lt;showing “Shuffling Gaussian Mechanisms / Privatizing Ads” deck>

Sivkanth: shuffling mechanisms as a way to privatize ads. A more detailed account of how it works and Differential Privacy (DP). Serving ads requires sending a lot of private user information to the ad network. The question is how we make ads relevant to the user while maintaining privacy. Privacy is quantifiable using DP to measure how much privacy a user is losing.

To introduce DP, it gives the guarantee that if you have two databases that differ by one user and you run a DP algorithm on the two databases, the output looks very similar. It will be very hard for you to determine if the particular user is present or not. A de facto form of privacy standard used by the industry, e.g. Google, Microsoft, US Gov’t for US Census data. Well-studied.

The setup is as follows: user info X, ad info Y– how do we tell which ad is relevant for each user? Train two neural networks. User embedding network, ad embedding network. Trained ahead of time. These two vectors will have a large dot product if there’s relevant, otherwise small dot product. The embedding vectors have norm of at most 1. You can either clip them or train them so that they have a small norm.

How do we privatize this? Introduce trusted mediary. Instead of sending embedding to ad network directly, add some gaussian noise. You add sigma times 1 to each coordinate and send it across to the ad network. Instead of giving the closest vector, give a number of closest neighbors. Give nearest neigbhor ads. The truth is somewhere around there. Trusted mediary picks the closest ad. This privatizes the user info, but to get meaningful privacy we need to add noise where sigma is about 1. If 0.2 or 0.3, the privacy you get becomes quite bad; add a lot of noise. But if you add a lot of noise, it becomes useless for ad selection.

What’s the utility loss that we see? For any ad embedding vector _u_ with unit norm, we have &lt;math equation explanation>. What that means is the error in dot product is about sigma; it doesn’t scale with the dimension (which is good). Want to reduce the noise scale to 0.2 or 0.3 but still have good privacy.

Smaller sigma => higher utility/accurace, larger sigma => better privacy.

If you just add noise and send it over, it’s not good enough– how do we improve the tradeoff? A trick called shuffling. Instead of sending one user’s ad embedding, collect many users’ ad embedding (e.g. 1000 users’ vectors). Batch 1000 user requests, add noise, and shuffle them. Instead of sending them directly where the ad network can tell whose is whose, it can’t tell; shuffling amplifies the privacy by making it hard for the ad network to detect a particular user’s vector. Shuffling decreases sigma by a factor of root 2 log n.

Jonasz: if no shuffling, how would the ad network determine the user the vector is from?

Sivakanth: might have some other info, e.g. a temporal join. Even if there’s some collusion between web site and ad network, still gives you privacy because of shuffling.

Brian May: if I have a user-based vector and an advertiser vector, how do I know there’s anything common between them? I have a vector of user behavior based on user events, an ad vector based on ad characteristics, how does what I’ve seen in the browser correspond with what’s in the ad vector?

Sivakanth: we train two neural networks– ad embedding neural network and user embedding neural network. If their behaviors are correlated, the dot product is high.

Brian: what is the overlap between the browser and the advertiser that lets you map them?

Shaoyu: the question is how we do the model training to make the vectors relevant. Multiple ways to do it. Hosted mediator could do the training. Ad network could get model trained by trusted mediator to generate the ad embedding. This is just one approach; can also use multi-party computing to do the training without disclosing too much info to each other.

Brian: more straightforward question– inputs from the browser are what sites the user goes to… what info comes from the ad network?

Joel: we’re saying there’s a modeling space that summarizes info in such a way that the distance after you project to this inner product space are close to each other. User history might indicate they’ve looked at running shoes, the site is running shoes, etc.-- the model minimizes the distance between two training spaces.

Brian: right, if advertiser says this ad is about this thing, there’s not a lot of correspondence between two things to indicate running shoes overlap with running shoes, but sports enthusiast may not have overlap.

Joel: we think you’d be able to do a pretty good job…

Brian: other question– shuffling phase. Thought about how folks would do things like A:B testing?

Joel: you’re saying with a different model and then sending? Or a different amount of noise and testing that? What test?

Brian: testing the efficacy of one creative vs. another. Does one subpopulation respond to one creative better than another. If there’s shuffling I have no way of knowing…

Joel: shuffle goes to ad side and you’re saying you want to divide the shuffle into two pieces. Say we have an A:B test, a bit flag entered/set by a certain set of ads and measure the impact of such a change?

Brian: yes.

Joel: we haven’t thought of that, but as long as you have enough data there’s no reason you couldn’t have different batches sent with a given bit. But Gopi/privacy folks might need to help reason about if too much is leaked. But a good point, thanks!

Michael Kleber: I was hoping to understand the shuffling a little better and the threat model. I’m down with the timing attacks, I keep bringing it up over and over again every time there’s a new variant of PARAKEET. As I understand it, the user navigates to a web page, triggers a PARAKEET auction that triggers a user’s embedding vector to go to a server, noised and batched with 999 other users, and ad network gets a batch of 1000 vectors. Even if the ad network does a timing attack, they don’t know which user vector was mine. In high dimension, most vectors are perpendicular to each other. The noise you’re adding will lower dot product from 1 to something like 0.7 or 0.8. What if there are two different ad requests coming from me at two different times? Ad tech seeing a request in one particular batch might say I think Michael Kleber is in this batch but I don’t know which one. The next request I can look at that second batch and compare to the ones in the prior batch. The dot product overlap between those vectors should stand up like a sore thumb. Even if I can’t figure that out, the third one I definitely can.

Sivakanth: we did worry about repeated requests. What I showed today was one request. To reduce the impact of such attacks is caching. If the same user makes repeated requests in the same say and the vector is the same, we don’t repeatedly send it to the ad network. We’ll use the cached ads we got back from the first request and not issue a new one. Within a day, we think it’s largely solved. Across days it may be a challenge.

Michael: I may visit the same user across 2 or 3 days, and probably possible for me to figure that out. So, you can track someone if they go to the same site across multiple days. There’s a lot of reliance on this cache and how this works.

Sivakanth: to give some intuition on why shuffling improves privacy– suppose an adversary is trying to differentiate between two variants. One user who sends a value that is a one or a zero– everyone else is sending out zeroes (with gaussian noise and shuffling). The ad network should not be able to distinguish between the two distributions. If sigma is small, the best strategy for adversary is to compare the max of the two samples. &lt;more description of math behind it>.

The approximation is widely used with DPSGD which is used to train neural networks with privacy. With this approximation, we can use the software “PRV Accountant” ([https://github.com/microsoft/prv_accountant/](https://github.com/microsoft/prv_accountant/)). Can calculate how much privacy loss is happening. &lt;shows graph of epsilon on Y axis and sigma on X axis with two different n values>.

False Positive Rate vs False Negative Rate curves. FPR is the probability that an adversary thinks the user is present when they’re not. And FNR is when they think they’re not present when they are. &lt;shows graphs vs. no privacy loss>. Say I add a sigma of 0.3, we get this red line which is pretty good– the adversary is quite confused. To reduce it to a single metric, we come up with a “guess probability” where the user is present with a probability of 0.5. 0.5 is trivial. See this graph where we’re 0.7 which is pretty good. Punchline: shuffling with n= 1000 with sigma = 0.25 is about the same as a single gaussian mechanism with sigma of 1.

We’ll show experiments that shows the difference in utility.

Zhisong Wang (MS Ads): I’ll talk about differential privacy for PARAKEET v2 noisy ranking. We talked about DP concept, but to briefly recap. DP allows us to quantify privacy loss. &lt;goes over some equations>.

We can apply gaussian mechanism to satisfy DP.

PARAKEET v2 architecture overview– can transfer user data into low dimensional embeddings. The trusted mediator can be hosted by a browser vendor or potentially independent third party. For model training, trusted mediator applies DP by using DP-SGD algorithm to create and maintain mapping between user signals and ad signals. For ad retrieval, applies DP by placing noisy embeddings in a batch with shuffling to send them to the buyer. For ad ranking, the ™ uses the true user embedding without noise to compare ads provided by the buyers for ranking and returns the winner ads from each buyer to the browser.

For each browser request, TM infers user embedding, adds noise, and batches to send to a buyer. The batch is sent to the buyer which does ad lookups for each vector and returns a collection back. The TM runs the logic provided by each buyer to evaluate ads. Various variables for the DP mechanism. Mediator is charged with setting values based on a community-approved bar.

Offline precision/recall result for a noisy sigma. In the first result, we kept the top 100 clean user embeddings and compared to top 1000 noisy user embeddings. Then varied noise scale (sigma) from 0.03 to 0.40. Compared precision and recall, comparing to unnoised. As we increase the noise scale, precision and recall decrease.

If we fix the noise sigma as 0.3 and the noisy user embedding to top 1000 while varying the top K orders for the clean embeddings.

If we keep noise scale fixed to 0.3 and make clean user embedding to top 100, but increase top K orders from noisy user embeddings– as we decrease K, precision goes up and recall goes down. Recall more important.

Here’s some online metrics from Microsoft Audience Network. In this setting we have two traffic groups. In treatment flight, we used user embeddings with noise added while in control flight we used them without noise added. Compared revenue. No significant difference in revenue or click through rate. This shows that in this setting, after adding noise we still don’t have a significant difference. Demonstrates we can improve privacy and maintain relevance.

Brian May: a lot of dependency on trusted mediator to adjust things appropriately to maintain both utility and privacy. Any ability for anyone on each side of the transaction that the TM is doing what it’s supposed to be doing?

Zhisong: good question– we’re doing an initial experiment.

Joel: we are anticipating it would run in a TEE of sorts. Code reviews. TEE enforces based on various things. Could validate that what’s on the TEE is running what it says it is, could sign outputs to validate it’s running what the TEE should be.

Brian: answers the question to the degree that I trust the TM is running appropriately. But TM could do something wrong on purpose or by accident. Unless I have some way of sending in a test packet/query/whatever the appropriate language is and seeing that I get back what I expect, I have to take it on faith that everyone has done everything right.

Joel: we don’t have an exact workflow on how to solve this. Folks may have ideas for “break glass” or debug workflows. Could potentially have some zero knowledge proof for some aspects. Need to iron out more details.

Michael Kleber: I’m thinking about this as a FLEDGE vs. PARAKEET delta always. I would like to put in a plug for TPAC having a meeting in Vancouver in 2-3 weeks. Probably on the Friday of TPAC. Thanks for mentioning the TEE proposals on our end. This is a different approach than that one because, in the Chrome privacy sandbox version of this, the trusted servers are generally run by ad tech. Usually the person who stands up the server running in side their Amazon instance and have it doing stuff. Actual ad tech company who is having things served. Only instance where we have an instance running on behalf of the browser is when its goal is to enforce privacy protections such as k-anonymity; sure, the browser runs something inside a TEE to make sure every ad is being shown to enough people so it can’t be used for tracking. Other than that, though, the things running inside of TEEs are run by ad techs. The biggest outstanding difference between FLEDGE and PARAKEET which we don’t have time to chat about today but should in the future, is the trusted server acting as a proxy; in PARAKEET case there’s info such as the gaussian mixing that gets an untrusted server afterward. In FLEDGE we let servers into the mix, there’s a bunch of relatively updated explainers such as the key-value server being able to run arbitrary code. More experimental proposal for both run ad auction and calculation to run on a server, but again not with a goal of proxying to an untrusted server which opens up timing attack and heightened risk. Rather, it’s trusted servers to offload computation from the device but only communicate back and forth to the browser so we don’t have to worry about new info channels. Where we were limited on on-device computer we can move some onto the cloud.

I know this gap is the largest, fundamental difference between FLEDGE and PARAKEET philosophical approach. I would love to have a discussion on which is more important to you. Run things from the browser into a trusted enclave environment in the cloud has a lot of potential that we’re exploring on the FLEDGE side (though not obvious how it will all work), but a lot of possible flexibility. But this part where we proxy things to an untrusted server seems like a fundamental difference that continues to scare me and unclear if we can make it safe in the face of multiple requests. I would love to understand better if a meeting of minds on the two is enough to get to compatible cross-browser, harmonious agreement. Or if something essential about how you want it to work requires the TM, I would love to chat more in the future.

Joel: I agree that’s the major difference. Our overall goal is to have the least amount of compute in the TEEs as possible. Offloading a lot of things to something untrusted but still have reasonable RPM and staying at today’s norm with small amounts of overhead/more expensive compute space with minimal infra changes. But, yes, we’re out of time! Should have a more dedicated meeting on this.

Michael: yes, I would love to dive into this more in a more dedicated time.

&lt;out of time>


### [Native ads update](https://github.com/WICG/privacy-preserving-ads/blob/35da18bbda75708780109a522778829355debf94/FencedNativeRendering.md)
