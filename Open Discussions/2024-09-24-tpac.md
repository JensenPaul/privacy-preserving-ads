# Ad Selection API and Protected Audience API Discussion @ TPAC
#### Tuesday, September 24th at 11pm CET / 5 pm EDT /  2 pm PDT

Scribe: Ben Kelly

## Agenda
* Review, identify, and prioritize potential topics
* Fabian Höring On-device vs server side comparison & future work

## Attendees
* Brandon Maslen (Microsoft Edge)
* Erik Anderson (Microsoft Edge)
* Sarah Murphy (Microsoft Edge)
* Isaac Foster (Microsoft Ads)
* Fabian Höring (Criteo)
* Michael Kleber (Google Chrome)
* Charlie Harrison (Google Chrome)
* Shivani Sharma (Google Chrome)
* Paul Jensen (Google Chrome)
* Aloïs Bissuel (Criteo)
* Lionel Basdevant (Criteo)
* Maxime Vono (Criteo)
* Elias Selman (Criteo)
* Fatma Moalla (Criteo)
* Priyanka Chatterjee (Google Privacy Sandbox)
* Sid Sahoo (Google Privacy Sandbox)
* Zainab Rizvi (Google Chrome)
* Junseok Lee (Samsung)
* Aram Zucker-Scharff (The Washington Post)
* Andrew Pascoe (NextRoll)
* Alexandra Reimers (Google Chrome)
* Ben Kelly (Google Chrome)
* Martin Thomson (Mozilla)
* David Dabbs (Epsilon)


### Notes:
* kleber: rules setting, scribing, agenda, etc
    * protected audience API by chrome
    * vs the ad selection API edge variant in the same space
    * working on unifying them (they are already similar)
    * will mostly talk about where these APIs differ
    * Chrome and Edge both have notes docs for our respective meetings!  We are using the Edge doc today
    * also doing agenda bashing at the beginning of the meeting
    * (construction noises…)
* agenda bashing:
    * kleber: what should we spend two hours talking about?
    * Erik has slides for some candidates to talk about
    * erik: going to review these briefly, but save discussion until later
        * merging repositories (links in slides)
        * process related topics (WICG, PAT CG)
        * TEE flows (its a bit separate from browsers, but tightly integrated)
        * on-prem datacenter support
            * google has some requirements published and edge has some early thoughts
        * ad selection API vs protected audience API
            * key-value lookup vs criteo variant
        * criteo proposed topics (see slides)
    * erik: open it up for discussion on prioritizing topics
    * kleber: how are we managing the queue?
    * erik: google docs, zoom hand raise, physically raising in room
    * Fabian from criteo: we have presentation on all these topics to bring people up to speed
    * kleber: one difference between APIs is that the chrome version envisions two ways of running actions instead of just one
        * the way the javascript performs the bidding and selection process can happen either 1) in the browser in a worklet or 2)  in a later design in a TEE with a bunch of services run by the buyer and the seller (bidding services)
        * the edge version of the API is focused only on the server-side variant so far
        * the bidding services approach is motivated by aligning these two APIs
        * believe that Fabian from criteo is interested in this topic
        * should we start here? (yes)
    * &lt;passing mic to Fabian>
    * Fabian: &lt;presenting slides>
        * "protected audience on-device vs server side"
        * starting with some history
            * 2020: turtledove on-device
                * joinAdInterestGroup()
                * runAdAuction()
                * runs fully on device
                * pros: private, users own data, totally on-device
                * cons: high latency from cpu load, lose features like budget capping
            * now: real time key value service
                * runAdAuction(), but execution happens off device in a TEE
                * TEE must be: 1) side-effect free, 2) no-network/disk/timers/logging, 3) look up in memory, 4) offline processing
                * pros: bidding can be capped, servers can handle performance costs better, simple API
                * cons: more complexity, infra cost, still has high latency from network call to the server
            * >2023: bidding and auction services
                * getInterestGroupAdAuctionData()
                * sends auction blob to servers via network fetch
                * server triggers runAdAuction
                * pros: reduced latency, better allocation of resources
                * cons: even more complexity, potentially high latencies due to blob payload sizes
                * &lt;very small diagram I can't read, see slides>
            * ROMA engine (no open source requirement for adtechs)
                * based on a server-side v8 sandboxed context
                * pros: allows to run proprietary code, engine can be stable
                * cons: lots of overhead from the sandbox
                * criteo implemented a KV service ROM engine benchmark
                    * KV service can handle 1000s of QPS with 5 to 8 ms latency
                    * asp.net scaled 10x better than ROMA
                    * this 10x penalty is theoretical, in practice it will probably be 3x to 5x with real workloads
                    * c# wasm could only handle 1 QPS (maybe C# vs wasms integration due to GC)
                * question from Isaac: can you explain what the benchmark is exactly?
                * Fabian: we implemented the KV server because its the simplest workload, much easier than bidding/auction service, but we ran it on top of the ROMA engine
                * Isaac: this is the default handler that your using?
                * Fabian: we implemented some javascript code with a fake campaign lookup, with some fake values loaded into the state, and then the queries are looked up from these fake values in memory
                * Isaac: but the second one, with asp.net, handles 10X queries… is this the same code or totally separate?
                * Fabian: we did basic asp.net controllers, but then the exact same logic for KV service elements with the same dummy/fake logic/data, this has the overhead of the asp.net, but the state was all in-memory
                * Isaac: so, the asp.net application does not replicate the sandboxing?
                * Fabian: correct, its not doing any sandboxing or TEE, but its the same application logic and data
                * Isaac: but still running in an enclave?
                * Fabian: no, none of this ran in enclave
                * Isaac: but this is normal adtech?
                * Fabian: yes, normal adtech to get a "skyline" or baseline
                * kleber: it sounds like the ROMA engine overhead is the significant bit here
                * Fabian: we estimate the enclave overhead at 20% to 30%, but much less than 10x/5x, etc
                * Isaac: agreed, the enclave should not be significant, but maybe the resetting or in-memory querying could be a factor here (might have missed this here ???)
                * Fabian: this is intended as one datapoint
                * Andrew Pascoe: is this test just one server?
                * Fabian: yes
                * Andrew: when we tested this sort of thing we had scaling issues due to sharding
                * Fabian: yes, we have not gotten there yet, but sharding needs to be tested
                * Lionel Basdevant: you did some tests with multiple servers and sharding. What did you see?
                * Andrew: yes, last year we tried running the open source KV source, but he had to implement our own server and the number of servers that need to communicate scales at N^2 due to trying to obfuscate server to server comms
                * Fabian: more discussions on sharding are needed to align everyone and to build confidence, calling N-to-N nodes does not look viable
                * kleber: we think the early thinking on N-to-N is the naive approach and a better solution is not implemented yet in our example code, in the long term we expect better sharding approaches to be possible
                * Andrew: is this public knowledge that we're trying to improve the sharding approach?
                * kleber: yes, we've said we want a better solution than all-to-all
            * Fabian: efficient in-memory caching
                * bidding system and then KV system state
                * property of this system means that the state must be serialized and deserialized for each lookup
                * responses must be within 50ms (for criteo, but probably for everyone)
                * network and IPC calls should be minimized
                * had over 30 caches in product systems for protected audience PoC
                * use case: efficient filtering, sending back campaigns based on country/geo based on intersection at the bidding layer, would be nice to have a simple reverse lookup cache keyed by country, etc, this should not be a privacy problem
                * use case: ML inference sidecar, separate process requiring IPC to perform inference, expect overhead to be pretty significant, benchmarks with ONX show a 5x penalty, caching could be a solution
            * Fabian: infra cost can get out of control
                * JS ROMA vs native c# is 5x
                * no shared memory is a high cost
                * ML inference side car is 5x
                * TEE and encrypted networking is +20%
            * Fabian: ideas for future work
                * shared memory
                * inlining data structures
                * new ROMA bring-your-own binary to allow languages beyond JS/WASM
                * common benchmarks, published results that are reproducible, improvements should be tracked
                * don't forget on-device
                    * ability to flexibly move workloads from service to on-device
                    * ability to A/B test on-device vs server
                    * exploration (rollout scaling via parallelization, metrics, CPU watchdog)
                        * resources need to be fairly allocated between all auction participants
            * David (Dabbs): running tests to compare on-device vs server… is there a process defined to ensure there is a common denominator between the different implementations so comparisons are apples-to-apples, or rather, do we need that?
            * kleber: to clarify, are you asking if chrome has feature parity between on-device vs on-server?
            * David: yes, want to have comparisons are valid
            * kleber: of course we aim for feature parity, but the implementations in chrome happen on-device first and then come to server later, so on-device tends to have more features than server implementation today, we'd love to keep them in sync but its not the case today because the server-side approach is newer and more experimental
            * David: Fabian referenced the promising bring-your-own binary approach for ROMA, but the sense I had from previous meetings is there was a commitment to that, is that the case or am I confused?  Are we committed to BYOB?
            * Isaac: are you directing this question to google?
            * David: whoever is driving the bus on the server side, so I guess primarily the chrome team
            * kleber: I don't know the answer to this off the top of my head, but maybe my colleagues can
            * David: ok - maybe we don't have the right people in the room
            * Isaac: I don't think I've seen any document updates in the repo
            * kleber: I'm sorry, but we don't have the BYOB folks in the room
            * kleber: edge folks, do you want to talk about your thoughts here?
            * Isaac: we are similarly positioned, talking with our internal confidential computing experts and hope to have something more concrete soon
            * Priyanka: can you repeat the BYOB question?
            * David: I was under impression that we/Chrome were committed to the gVisor/BYOB approach - that this is where you are sailing
            * Priyanka: the current plan is in our recently updated explainer, we plan to have BYOB in the Bidding server by November 2024, we are in the process of updating some of the guides, in the process of updating B&A roadmap. Starting November, both ROMA BYOB and Roma based JS+WASM would be available as options for adtechs.  (I think I missed a fair bit here as scribe)
                * TODO(Priyanka): Will link in the generateBid() BYOB guide for Bidding server by 9/27
            * Explainer for Chrome's BYOB work: [https://github.com/privacysandbox/protected-auction-services-docs/blob/main/roma_bring_your_own_binary.md](https://github.com/privacysandbox/protected-auction-services-docs/blob/main/roma_bring_your_own_binary.md)
            * Fabian: you will support BYOB in the bidding service, but not the KV service?
            * Priyanka: we will support, but on different timelines, see the roadmap updates on explainers 
            * Isaac: great to hear that, thank you, so it sounds like generateBid will be able to use, handleRequest will be able to use it, KV will be able to use it, and there is no objection to scoreAd using it, but its not a priority, is that correct?
            * Priyanka: yes, that's correct, want to hear from SSPs the reasons:  because of performance reasons partners may want to use BYOB. Some adtechs may want to use BYOB, since they don't prefer JS or WASM. But BYOB for Auction / scoreAd() is not prioritized due to resourcing at the moment. We will wait for SSPs to request us explicitly.
    * kleber: wrapping up this section, thank you for presentation from criteo and all participants on this topic
* kleber: other topics or questions?
* David: I saw a reference to a potential bridge capability between v1 KV frontend that could basically hit the KV server and dip your toe getting the KV2 setup running, but incurs some latency, is this real?
* kleber: yes, that is generally correct, do we have anyone from the team that knows the details or I can give an overview
* kleber: guess its me, at a high level in chrome the call to the KV server for now the KV server is not yet required to run in a TEE, we are in the middle of building out the long term future in which KV must be in TEE, the thing that is being asked about here is how to do the transition between the current support to the long term future version, the future version allows us to give more information to the server so there are motivations to move to v2, so people might be worth trying out the TEE KV, so to facilitate this transition we have a two hop solution that will go from untrusted-to-trusted so you can test both types of servers in one request, but I don't have the technical details to hand
* David: thanks for verifying that it’s a thing and it's in progress beyond an idea
* kleber: sorry, I don't have anything to announce about dates, its work in progress
* Fabian: can we spend five minutes talking about the benchmarking tool, should we publish it?
    * It would be interesting to know if there is a alignment that this is an important topic, from chrome or ad selection folks
    * will resources be spent on this topic?
    * criteo did benchmarks with gatling due to convenience
    * google has published a benchmark tool for the KV service, based on bazel, not easy to use
        * doesn't support bidding and auction services
        * can't load multiple instances
        * no agreement on how to share results
    * how can we move this forward? do we agree this is important? open question to the room…
* paul: we think benchmarks are important and we have a number of them for on-device side of things
    * I can't answer for the B&A team, but I assume it will be quite useful
    * previous benchmarks on github has been great in the past to get different folks aligned about problems
    * I'm a big fan of benchmarks
* Fabian: what can you share here or offline?
* Priyanka: We have published scalability guidance in explainer, open source load testing tools that will discuss how to get benchmarks. We are probably also going to publish a benchmark tool later in the year, talking about B&A here, it's in the works
* Fabian: what is the edge view?  will contribute resources?
* Erik: I think we are similar to Paul, we think benchmarks are super helpful to understand changes, detecting regressions, etc.  I don't know how this will play out over time in terms of who should be producing the benchmarks, where they should run before a build goes out, but we seem to have interest, should there be common infra? \

* kleber: I'd like to propose a topic pivot now
    * we've had a lot of discussions between criteo, google, and microsoft
    * are there things other folks in the room want to call out
    * we would love to get input from a different browser (or chairs another group related here)
* Martin Thompson (mt): you know my position and nothing I've heard changes my position
    * mozilla published a position on this
    * &lt;two mics>
    * I don't think anything here that was discussed that would change that position
    * more important discussion would be on "can this be made private?"
* kleber: on the question of, can this be done in a private way, I think the chrome position is that it was willing to start with a non-perfect solution that we acknowledge has not met the privacy goals, the things you've called out in the mozilla position paper are definitely things that are planned for the future, and understand mozilla doesn't want to do non-private things now with the promise of change in the future
* mt: but is it possible to achieve the goals in the end?  it seems that it is likely not possible, we may disagree on the reasons, but it seems fundamentally impossible to meet the goals you've stated given the constraints, so its hard to support something that even if it were perfect its hard to get right and will likely have shortcomings
* mt: my question about what happens when you put an ad in front of someone, you include humans in the feedback loop, I think that continues to be a sticking point
* kleber: at a high level I'm grossly over generalizing, but I characterize your position paper on the chrome that there are two big parts with our implementation that are problematic from a privacy point of view:
    * the javascript execution environment where running arbitrary code on secret data is challenging
* mt: yes, that is technically possible but super super hard, probably possible, but not achievable in its current form, both TEE and on-device have independent technical challenges without a known solution
* kleber: so the choice to choose arbitrary js execution or BYOB for the mechanism for bids was partly due to expediency so we could understand how to do it, but that is not the only way this could be done, so maybe there is a path forward here where we settle on a declarative syntax that supports a more private solution, since a declarative syntax could have known privacy properties
* mt: yes, but its likely going to be extremely constraining to adtech utility, but this is only one part of the problem
* mt: the real problem is you need to put an ad on the page where this is not an ad, and at some point someone is going to be aware of that ad, and this leaks information from the ad to the publisher
* kleber: so there is a difficulty at rendering time, there is the networking privacy problem, but then there is the rendering part of the problem that a page framing the ad implicitly allows information to be shared, this is the fenced frame problem, and you are skeptical its solvable
* Erik: do you anticipate mozilla proposing something in the ad serving realm or is this not something browsers should be doing?
* mt: nothing as absolute as browsers shall-not-do, its never that simple
* Erik: trying to parse the anonym blog - [https://blog.mozilla.org/en/mozilla/mozilla-anonym-raising-the-bar-for-privacy-preserving-digital-advertising/](https://blog.mozilla.org/en/mozilla/mozilla-anonym-raising-the-bar-for-privacy-preserving-digital-advertising/) 
* mt: not saying it *shouldn't* be done, but more the case we don't have a solution for making it safe, the threat model means we have to consider the worst way an API can be used and the current approaches seem to have a lot of concerns here
* Aram: do you believe the fenced frames proposal will not work?
* mt: yes, I think its incapable of achieving its goals
* David: What you just said Martin seems similar to what was said regarding a new shared storage proposal for supporting certain payment providers
* mt: yes, there is a google blog post about showing the last 4 digits of a credit card number,which I think is deceptive all by itself, but actually there are much worse things you could do with this API
* Shivani: to clarify fenced frames issues, when a user clicks on an ad and then there is a server side interaction, is that the main concern with fenced frame?
* mt: that is part of it, I've spoken with kleber about some more interesting ones, in general any interaction with that page can leak information that was secret, because information is being viewed by the user and therefore could influence actions
    * for example, if you put some text in the ad and then have a form next to it asking the user what is shown, captchas have taught users to do this, an easy way to exfiltrate data
    * mouse pointer information is another vector
* Shivani: to summarize, there must a user taking that intentional action, so that seems like the boundary of the threat model is OK if you're just looking at programmatic behavior, but not at human behavior?
* Martin: &lt;nodding>
* Aram: is it possible for the fenced frame to remove interactions to address this?
* mt: no, because the human can read the info in the fenced frame and change their behavior, and there are probably other nasty things that can be done here
    * if you've looked at AR/VR tech is there is the ability to track user eyes and this can result in unexpected abuses
* mt: you've broken the seal by putting it on the screen
* kleber: thank you for the discussion
* mt: what are you going to do about it?
* kleber: of the three hard things we talked about, the isolating of processing of secret data is solvable, the loading info over the network is hard won't be solved quickly, putting things in TEEs might seem like an approach but its not easy
* kleber: on the fenced frame rendering side, I think Shivani has pointed out the boundary that allows chrome to proceed here, we see a difference between protecting against programmatic attacks vs protecting against user actions
    * it seems like these should be thought about in different ways
    * we think the programmatic approach is a more scalable threat
    * human-in-the-loop vectors seem more limited in scale
    * this is the beginning of the story, thought, and it seems like you are right that there is a need to distinguish between trustworthy and untrustworthy content
    * this is a topic other groups at TPAC are discussing (social media, genAI, etc)
* mt: those seem like very different problems (social media, genAI, etc)
* kleber: they do have differences, but the underlying issue that not all pixels on the screen can be trusted equally is shared, consider history around line-of-death (the line of what the browser owns vs what the website owns, and making sure that is clear), it seems like what you are describing is a more complex version of this same problem, that doesn't mean the problem is unsolvable and we might need a solution for other use cases as well, its probably a bigger problem for genAI in social media than malicious ads here, there are UI things that could be done here to highlight ads, etc
* mt: "this is a box with an ad in it" is an extreme option, but its not a very good one, it seems like that might be appealing because it constrains what the advertising industry can do
* Isaac: can you clarify?  are you referring to native?
* mt: what does native mean?
* Aram: it means the ad looks like content on the page
* Isaac: there are number of things where adtech companies will choose to have a choice on the screen, so if the two problems are philosopihically different, if we have a red line around ads, that says "AD AD AD AD AD" all along it, this might be less of a problem than you think
* Aram: there are two things in this discussion
    * 1) I agree an intrusive ad frame is probably not a problem, advertisers will become accustomed or they won't and bid on a different type of ad.  This is probably attractive enough to advertisers that they would not mind, but I also don't have a problem with some ads going away.
    * 2) In the question of BYOB vs declarative, we've talked about a lot of features that SSPS and others bring to the table, but the tradeoff here seems that they could adapt to a much more limited system.  As a publisher it seems plausible.  I wonder if something as structured as prebid model could be effective here
* Isaac: what part of the system are referring to being more constrained
* Aram: what we were talking about replacing BYOB
* kleber: I think we are talking about declarative syntax here
* Isaac: I think what you are saying, and is the fundamental tradeoff, is that there will be a great deal more pain operationally, but if the data is powerful then the pain may be acceptable, where the balance is remains to be seen, though,
    * in regards to BYOB vs declarative, that one seems tricky to me
* Aram: there are different use cases, 1) declarative language might be close to what publishers like, but 2) SSPS might want more flexibility
* Isaac: on the sell side you presumably would care, even if the declarative language is perfect, you would still care if the buy side can't do what they want to do
* Aram: yes, but I think there is a lot of advantage here
* Isaac: I agree, its the same tradeoff, I think its an interesting question if we can have a competitive ecosystem on the buyside and sell side with only a limited domain specific language
* kleber: thank you for the discussion, I think the prospect of a DSL is only relevant if it is the only thing preventing other browsers participating, but this discussion suggests there are other larger things to disagree
* mt: if that was the only difference, then yes we could align, but its not the only issue
* Isaac: if we agreed on the rendering piece, and the firefox said it had to be some DSL, then maybe
* Shivani: going back to human-in-the-loop, that threat model does not seem as scalable as programmatic threats, its also not very deterministic, should we be considering a tradeoff here vs requiring perfection?
* Isaac: it depends on what your threat model is, if you want to make it so a motivated attacker can't find any information vs scaled tracking across tons of users, etc
* kleber: yes, its a useful discussion, but captchas show that it is indeed scalable and easy to get users to type things on the screen, we've already trained them to do this, so I sympathize that this could/should be in the bounds of the threat model, it doesn't seem that we should argue that its hard to get users to type things
* Isaac: to be clear, I don't think Shivani or I were saying it was "beyond the pale", but that there is some tradeoff here that different browsers could draw a different line on, different systems could make different choices on tradeoffs
* mt: its not necessarily the responsibility of the people critizing a proposal to establish whether a threat is scalable or not, its more a question whether there is a proportionate response that could be taken to that threat, and I'd rather not see it dismissed out of hand
* kleber: very reasonable
* kleber: we have 10 minutes left, no current topics or hands, should we call it?
* fin
